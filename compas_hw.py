# -*- coding: utf-8 -*-
"""compas_hw.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/169-Kj_JoP9fnJAMIF5-MwTXtB_0d6RRX
"""

# Update with your NetID
netid = "vgarg"

# Loads the data and splits the labels (target output) from the input
import pandas as pd
data = pd.read_csv("https://raw.githubusercontent.com/DikshaShukla/uw-ml-dataset/main/fake.csv", index_col=0)
labels = data.pop("recidivist")

# Extracts features from raw data
# replaces non-numeric categories with one-hot encodings
features = data.copy()
features["male"] = (features["gender"] == "Male").astype(int)
features["female"] = (features["gender"] == "Female").astype(int)
features = features.drop(columns="gender")
features["african-american"] = (features["race"] == "African-American").astype(int)
features["caucasian"] = (features["race"] == "Caucasian").astype(int)
features["hispanic"] = (features["race"] == "Hispanic").astype(int)
features["asian"] = (features["race"] == "Asian").astype(int)
features["native american"] = (features["race"] == "Native American").astype(int)
features["other"] = (features["race"] == "Other").astype(int)
features = features.drop(columns="race")
features = features.drop(columns=["recidivism risk score", "first name", "last name", "full name", "risk assessment date", "date of birth"])

# Display the data
data

# Display the features
features

# False positives and negatives based on recidivism risk score
def assess_risk_score(data, labels, cutoff):
  """
  Assesses the following rule based on COMPAS risk score to predict recidivism:
    If the risk score is greater than the cut-off,
    then predict that the individual is a recidivist.
  A prediction of 1 means the criminal will be a recidivist
  A prediction of 0 means the criminal will not be a recidivist
  Measures prediction performance using the true labels from the data.
  Returns a tuple (fp, fn, er):
    fp: the number of false positives (wrongly predicted recidivism = 1)
    fn: the number of false negatives (wrongly predicted recidivism = 0)
    er: the error rate (number of false predictions divided by size of dataset)
  """
  false_positives = 0
  false_negatives = 0
  error_rate = 0
  for i in range(len(data)):
    if data.loc[i].at["recidivism risk score"] > cutoff and labels[i] == 0:
      false_positives = false_positives + 1
    if data.loc[i].at["recidivism risk score"] <= cutoff and labels[i] == 1:
      false_negatives = false_negatives + 1
  
  tot = false_positives + false_negatives
  error_rate = tot/len(data)




  return false_positives, false_negatives, error_rate

# Prints and plots the results
x, y = [], []
for cutoff in range(11):
  fp, fn, er = assess_risk_score(data, labels, cutoff)
  print((cutoff, fp, fn, er * data.shape[0]))
  x.append(cutoff)
  y.append((fp, fn, er * data.shape[0]))

import matplotlib.pyplot as pt
pt.plot(x, y)
pt.xlabel("cut-off")
pt.legend(["fp","fn","num errors"])

# Training a neural network to predict recidivism
import numpy as np
import torch as tr

# Helper function converts pandas dataframe to pytorch tensor:
def to_tensor(df): return tr.tensor(df.values.astype(np.float32))

inputs, targets = to_tensor(features), to_tensor(labels)

# Neural network class definition
class Net(tr.nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    # The number of neurons in the hidden layer should be twice the length of your NetID
    # l1 is the layer from input to hidden
    # l2 is the layer from hidden to output
    # Both should use the Linear module in torch.nn
    self.l1 = tr.nn.Linear(10, 10)
    self.l2 = tr.nn.Linear(10, 1)
    
  def forward(self, x):
    # Define the network's forward pass
    # For an input feature vector x, should return a scalar output y
    # y should be between 0 and 1, indicating the recidivism prediction
    # Use tanh for the hidden layer activation
    # Use sigmoid for the output node activation
    
    y = tr.tanh(self.l1(x))
    y = tr.sigmoid(self.l2(y))

    return y


# The following runs gradient descent on the data
net = Net()
optimizer = tr.optim.SGD(net.parameters(), lr=0.01/inputs.shape[0])
num_epochs = 10000
for epoch in range(num_epochs):
  # Start with zero gradient
  optimizer.zero_grad()

  # Calculate network output and sum of squared loss for each datapoint
  y = net(inputs)
  loss = ((y.squeeze() - targets.squeeze())**2).sum()

  # Calculate gradients and take a descent step
  loss.backward()
  optimizer.step()

  # Monitor optimization progress
  num_errors = (y.squeeze().round().detach().numpy() != targets.numpy()).sum()
  if epoch % (num_epochs/10) == 0: print(loss, num_errors)

# Print predicted and target outputs for the first 10 datapoints 
print(y.squeeze()[:10])
print(y.squeeze()[:10].round())
print(targets[:10].squeeze())
print((y.squeeze().detach().numpy() > .5).any())

# runs tests
fp, fn, er = assess_risk_score(data, labels, 2)
assert(fp == 1703 and fn == 165 and er*data.shape[0] == 1868.0)
fp, fn, er = assess_risk_score(data, labels, 6)
assert(fp == 481 and fn == 491 and er*data.shape[0] == 972.0)

n = Net()
assert(n.l1.weight.shape == (2*len(netid), features.shape[1]))
assert(n.l2.weight.shape == (1, 2*len(netid)))